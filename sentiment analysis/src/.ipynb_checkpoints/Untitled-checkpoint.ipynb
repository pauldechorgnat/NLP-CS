{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing packages \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# loading the stopwords library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(stems):\n",
    "    vectors = []\n",
    "    for stem in stems: \n",
    "        token = nlp(stem)\n",
    "        vectors.append(token.vector)\n",
    "    return vectors\n",
    "\n",
    "def padding_step(vectors, length = 36):\n",
    "    vectors = np.array(vectors)\n",
    "    dim_embed = vectors.shape[1]\n",
    "    num_words = vectors.shape[0]\n",
    "    diff = num_words - length \n",
    "    \n",
    "    if diff == 0:\n",
    "        return vectors\n",
    "    else:\n",
    "        if diff<0 :\n",
    "            diff = np.abs(diff)\n",
    "            if diff % 2 ==0:\n",
    "                return np.concatenate([np.zeros(shape = (int(diff/2), dim_embed)),\n",
    "                                       vectors, \n",
    "                                       np.zeros((int(diff/2), dim_embed))])\n",
    "            else :\n",
    "                return np.concatenate([np.zeros((int(diff/2), dim_embed)),\n",
    "                                       vectors,\n",
    "                                       np.zeros((int(diff/2)+1, dim_embed))])\n",
    "        else : \n",
    "            return vectors[int(diff/2):int(diff/2)+length, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "print(\"Loading data ...\")\n",
    "train_data = pd.read_csv(trainfile, sep = \"\\t\",\n",
    "                         names = [\"sentiment\", \"subject\", \"word\", \"timestamp\", \"original_text\"])\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# first lower the text \n",
    "print(\"Text tokenization ...\")\n",
    "train_data['text'] = train_data['original_text'].apply(str.lower)\n",
    "# parse the words\n",
    "# we want to emphasize that there are special care to take about the word not and its contractions: \n",
    "# it might be useful to keep them\n",
    "train_data['text'] = train_data[\"text\"].apply(lambda sentence: sentence.replace(\"can\\'t\", \"can not\"))\n",
    "train_data['text'] = train_data[\"text\"].apply(lambda sentence: sentence.replace(\"n\\'t\", \" not\"))\n",
    "train_data['words'] = train_data[\"text\"].apply(lambda sentence:  \"\".join((char if char.isalpha() else \" \") for char in sentence).lower().split() )\n",
    "print(\"Tokenization done\")\n",
    "\n",
    "# getting rid off stopwords\n",
    "print(\"Removing stopwords ...\")\n",
    "self.stopwords = stopwords.words(\"english\")\n",
    "self.stopwords.remove(\"not\")\n",
    "train_data['words'] = train_data[\"words\"].apply(lambda words : [word for word in words if word not in self.stopwords])\n",
    "print(\"Stopwords removed\")\n",
    "\n",
    "# stemming the words with a Porter Stemmer\n",
    "print(\"Starting stemming ...\")\n",
    "stemmer = nltk.porter.PorterStemmer()\n",
    "train_data['stems'] = train_data[\"words\"].apply(lambda words : [stemmer.stem(word) for word in words])\n",
    "print(\"Stemming done\")\n",
    "\n",
    "# performing word embedding\n",
    "print(\"Starting word embedding ...\")\n",
    "train_data['words_embedded'] = train_data['stems'].apply(get_word_embeddings)\n",
    "print(\"Word embedding done\")\n",
    "# averaging the word embedding for a given text\n",
    "train_data['avg_embedding'] = train_data['words_embedded'].apply(lambda x: np.mean(x, axis =0))\n",
    "\n",
    "# saving polarisation appart\n",
    "print(\"Starting final formatting of the data ...\")\n",
    "y = pd.get_dummies(train_data['sentiment'])\n",
    "\n",
    "# transforming the aspect data into dummies\n",
    "train_data = pd.get_dummies(train_data, columns = ['subject'])\n",
    "\n",
    "# getting rid of unnecessary data\n",
    "train_data = train_data[['avg_embedding',\n",
    "                         'subject_AMBIENCE#GENERAL', 'subject_DRINKS#PRICES',\n",
    "                         'subject_DRINKS#QUALITY', 'subject_DRINKS#STYLE_OPTIONS',\n",
    "                         'subject_FOOD#PRICES', 'subject_FOOD#QUALITY',\n",
    "                         'subject_FOOD#STYLE_OPTIONS', 'subject_LOCATION#GENERAL',\n",
    "                         'subject_RESTAURANT#GENERAL', 'subject_RESTAURANT#MISCELLANEOUS',\n",
    "                         'subject_RESTAURANT#PRICES', 'subject_SERVICE#GENERAL']]\n",
    "\n",
    "for i in range(300):\n",
    "    train_data[\"avg_embedding\" + '_' + str(i)] = train_data[\"avg_embedding\"].apply(lambda x: x[i])\n",
    "train_data.drop([\"avg_embedding\"], axis = 1, inplace = True)\n",
    "\n",
    "self.X = train_data.values\n",
    "self.y = y['positive']*1 + y['negative']*-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

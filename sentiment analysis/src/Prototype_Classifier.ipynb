{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing packages \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# loading the stopwords library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(stems):\n",
    "    vectors = []\n",
    "    for stem in stems: \n",
    "        token = nlp(stem)\n",
    "        vectors.append(token.vector)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_step(vectors, length = 36):\n",
    "    vectors = np.array(vectors)\n",
    "    dim_embed = vectors.shape[1]\n",
    "    num_words = vectors.shape[0]\n",
    "    diff = num_words - length \n",
    "    \n",
    "    if diff == 0:\n",
    "        return vectors\n",
    "    else:\n",
    "        if diff<0 :\n",
    "            diff = np.abs(diff)\n",
    "            if diff % 2 ==0:\n",
    "                return np.concatenate([np.zeros(shape = (int(diff/2), dim_embed)),\n",
    "                                       vectors, \n",
    "                                       np.zeros((int(diff/2), dim_embed))])\n",
    "            else :\n",
    "                return np.concatenate([np.zeros((int(diff/2), dim_embed)),\n",
    "                                       vectors,\n",
    "                                       np.zeros((int(diff/2)+1, dim_embed))])\n",
    "        else : \n",
    "            return vectors[int(diff/2):int(diff/2)+length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \"\"\"Le Classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, trainfile):\n",
    "        \"\"\"Trains the classifier model on the training set stored in file trainfile\"\"\"\n",
    "        # Loading the training data\n",
    "        print(\"Loading data ...\")\n",
    "        train_data = pd.read_csv(trainfile, sep = \"\\t\",\n",
    "                                 names = [\"sentiment\", \"subject\", \"word\", \"timestamp\", \"original_text\"])\n",
    "        print(\"Data loaded\")\n",
    "        \n",
    "        # first lower the text \n",
    "        print(\"Text tokenization ...\")\n",
    "        train_data['text'] = train_data['original_text'].apply(str.lower)\n",
    "        # parse the words\n",
    "        # we want to emphasize that there are special care to take about the word not and its contractions: \n",
    "        # it might be useful to keep them\n",
    "        train_data['text'] = train_data[\"text\"].apply(lambda sentence: sentence.replace(\"can\\'t\", \"can not\"))\n",
    "        train_data['text'] = train_data[\"text\"].apply(lambda sentence: sentence.replace(\"n\\'t\", \" not\"))\n",
    "        train_data['words'] = train_data[\"text\"].apply(lambda sentence:  \"\".join((char if char.isalpha() else \" \") for char in sentence).lower().split() )\n",
    "        print(\"Tokenization done\")\n",
    "        \n",
    "        # getting rid off stopwords\n",
    "        print(\"Removing stopwords ...\")\n",
    "        self.stopwords = stopwords.words(\"english\")\n",
    "        self.stopwords.remove(\"not\")\n",
    "        train_data['words'] = train_data[\"words\"].apply(lambda words : [word for word in words if word not in self.stopwords])\n",
    "        print(\"Stopwords removed\")\n",
    "        \n",
    "        # stemming the words with a Porter Stemmer\n",
    "        print(\"Starting stemming ...\")\n",
    "        stemmer = nltk.porter.PorterStemmer()\n",
    "        train_data['stems'] = train_data[\"words\"].apply(lambda words : [stemmer.stem(word) for word in words])\n",
    "        print(\"Stemming done\")\n",
    "        \n",
    "        # performing word embedding\n",
    "        print(\"Starting word embedding ...\")\n",
    "        train_data['words_embedded'] = train_data['stems'].apply(get_word_embeddings)\n",
    "        print(\"Word embedding done\")\n",
    "        # averaging the word embedding for a given text\n",
    "        train_data['avg_embedding'] = train_data['words_embedded'].apply(lambda x: np.mean(x, axis =0))\n",
    "        \n",
    "        # saving polarisation appart\n",
    "        print(\"Starting final formatting of the data ...\")\n",
    "        y = pd.get_dummies(train_data['sentiment'])\n",
    "        \n",
    "        # transforming the aspect data into dummies\n",
    "        train_data = pd.get_dummies(train_data, columns = ['subject'])\n",
    "        \n",
    "        # getting rid of unnecessary data\n",
    "        train_data = train_data[['avg_embedding',\n",
    "                                 'subject_AMBIENCE#GENERAL', 'subject_DRINKS#PRICES',\n",
    "                                 'subject_DRINKS#QUALITY', 'subject_DRINKS#STYLE_OPTIONS',\n",
    "                                 'subject_FOOD#PRICES', 'subject_FOOD#QUALITY',\n",
    "                                 'subject_FOOD#STYLE_OPTIONS', 'subject_LOCATION#GENERAL',\n",
    "                                 'subject_RESTAURANT#GENERAL', 'subject_RESTAURANT#MISCELLANEOUS',\n",
    "                                 'subject_RESTAURANT#PRICES', 'subject_SERVICE#GENERAL']]\n",
    "        \n",
    "        for i in range(300):\n",
    "            train_data[\"avg_embedding\" + '_' + str(i)] = train_data[\"avg_embedding\"].apply(lambda x: x[i])\n",
    "        train_data.drop([\"avg_embedding\"], axis = 1, inplace = True)\n",
    "        \n",
    "        self.X = train_data.values\n",
    "        self.y = y['positive']*1 + y['negative']*-1\n",
    "        \n",
    "            \n",
    "        self.model = SVC()\n",
    "        print(\"Data formatted\")\n",
    "        \n",
    "        print(\"Starting model fitting ...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size = .3)\n",
    "        \n",
    "        self.model.fit(X_train, y_train)  \n",
    "        print(\"Model fitted\")\n",
    "        \n",
    "        \n",
    "    def predict(self, datafile):\n",
    "        \"\"\"Predicts class labels for the input instances in file 'datafile'\n",
    "        Returns the list of predicted labels\n",
    "        \"\"\"\n",
    "        raise(NotImplemented('Implement it !'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DEV MODE #####\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Data loaded\n",
      "Text tokenization ...\n",
      "Tokenization done\n",
      "Removing stopwords ...\n",
      "Stopwords removed\n",
      "Starting stemming ...\n",
      "Stemming done\n",
      "Starting word embedding ...\n",
      "Word embedding done\n",
      "Starting final formatting of the data ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Classifier' object has no attribute 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cf9debf0abd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/traindata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-ebdaa15ae6d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainfile)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Classifier' object has no attribute 'y'"
     ]
    }
   ],
   "source": [
    "classifier.train(\"../data/traindata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = pd.read_csv('../data/traindata.csv', sep = '\\t', names = [\"polarisation\", \"1\", \"2\", \"3\", \"4\"])\n",
    "y = actual_values[\"polarisation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['negative']*-1 + y['positive']*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C = .35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.35, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization = {}\n",
    "\n",
    "for c in [.25, .5, .75, 1, 1.25]:\n",
    "\n",
    "    scores = []\n",
    "    model = LinearSVC(C=c)\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train = X[train_index, :]\n",
    "        X_test  = X[test_index, :]\n",
    "        y_train = y[train_index]\n",
    "        y_test  = y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        scores += [sum(predictions == y_test)/len(y_test)]\n",
    "        \n",
    "    optimization[str(c)] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.25': 0.777078626799557,\n",
       " '0.5': 0.7797430786267996,\n",
       " '0.75': 0.7790741971207088,\n",
       " '1': 0.7737519379844962,\n",
       " '1.25': 0.7737563676633444}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 3s 2ms/step - loss: 0.6982 - acc: 0.7116\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 1s 1ms/step - loss: 0.5257 - acc: 0.7930\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 1s 1ms/step - loss: 0.4556 - acc: 0.8238\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 1s 1ms/step - loss: 0.4052 - acc: 0.8454\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 2s 1ms/step - loss: 0.3568 - acc: 0.8595\n",
      "Epoch 6/10\n",
      "1203/1203 [==============================] - 1s 981us/step - loss: 0.3154 - acc: 0.8795\n",
      "Epoch 7/10\n",
      "1203/1203 [==============================] - 1s 894us/step - loss: 0.2683 - acc: 0.9044\n",
      "Epoch 8/10\n",
      "1203/1203 [==============================] - 1s 865us/step - loss: 0.2310 - acc: 0.9119\n",
      "Epoch 9/10\n",
      "1203/1203 [==============================] - 1s 1ms/step - loss: 0.1950 - acc: 0.9360\n",
      "Epoch 10/10\n",
      "1203/1203 [==============================] - 1s 1ms/step - loss: 0.1775 - acc: 0.9343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f178fe82b00>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape = (X.shape[1], ), activation = 'relu'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, pd.get_dummies(y_train), epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2,\n",
       "       2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 0,\n",
       "       0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2,\n",
       "       0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 0,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 270us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7156643708546956, 0.7833333325386047]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, pd.get_dummies(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e70231ba7c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m303\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/paul/Desktop/vector.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "plt.matshow(padding_step(train_data['vectors'][303], length = 36))\n",
    "plt.savefig('/home/paul/Desktop/vector.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.vectors.apply(len).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
